{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cada0f4",
   "metadata": {},
   "source": [
    "# Phenotype classifcation using CellX \n",
    "\n",
    "This notebook shows how to take segmented time lapse microscopy images and use h2b fluorescence markers to classfiy mitotic state of the cell cycle. \n",
    "\n",
    "The sections of this notebook are as follows:\n",
    "\n",
    "1. Load images\n",
    "2. Localise the objects\n",
    "3. Classify the objects\n",
    "4. Batch process\n",
    "\n",
    "The data used in this notebook is timelapse microscopy data with h2b-gfp/rfp markers that show the spatial extent of the nucleus and it's mitotic state. \n",
    "\n",
    "This notebook uses the dask octopuslite image loader from the CellX/Lowe lab project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c311e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopuslite import DaskOctopusLiteLoader\n",
    "import btrack\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "from cellx import load_model\n",
    "from cellx.tools.image import InfinitePaddedImage\n",
    "from skimage.transform import resize\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [18,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5906f",
   "metadata": {},
   "source": [
    "## 1. Load segmentation images\n",
    "\n",
    "#### *Important:* from this point on you will need to be consistent with the use of cropping and alignment. \n",
    "Using a previously generated alignment transformation will aid greatly in the tracking notebook, which depends on the object localisation performed in this notebook. Cropping your images will ensure that no border effects from the translational shift are seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ee9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cropping: (1200, 1600)\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "expt = 'ND0022'\n",
    "pos = 'Pos12'\n",
    "root_dir = '/home/nathan/data/kraken/ras'\n",
    "image_path = f'{root_dir}/{expt}/{pos}/{pos}_images'\n",
    "transform_path = f'{root_dir}/{expt}/{pos}/gfp_clipped_transform_tensor.npy'\n",
    "images = DaskOctopusLiteLoader(image_path, \n",
    "                               transforms=transform_path,\n",
    "                               crop=(1200,1600), \n",
    "                               remove_background=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccac2e",
   "metadata": {},
   "source": [
    "## 2. Localise the objects\n",
    "We need to also measure the mean intensity regionprops parameter in order to differentiate object class, for which we need to provide an image to measure. This means we need to provide the segmentation images twice: once to find the centroid and once to measure the pixel intensity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e488982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Channels.BRIGHTFIELD: 0>,\n",
       " <Channels.GFP: 1>,\n",
       " <Channels.RFP: 2>,\n",
       " <Channels.IRFP: 3>,\n",
       " <Channels.MASK_B: 94>,\n",
       " <Channels.MASK_A: 95>,\n",
       " <Channels.MASK_IRFP: 96>,\n",
       " <Channels.MASK_RFP: 97>,\n",
       " <Channels.MASK_GFP: 98>,\n",
       " <Channels.MASK_2CH: 99>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5f3d937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/03/30 06:56:33 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/03/30 07:02:30 PM] Objects are of type: <class 'dict'>\n",
      "[INFO][2022/03/30 07:02:34 PM] ...Found 446249 objects in 1098 frames.\n"
     ]
    }
   ],
   "source": [
    "objects = btrack.utils.segmentation_to_objects(\n",
    "    images['mask_2ch'],\n",
    "    properties = ('area', 'eccentricity'),\n",
    "    assign_class_ID = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa49b7b",
   "metadata": {},
   "source": [
    "#### Can also assign measured values from raw image to each segment using `skimage.measure.regionprops` parameters\n",
    "But also need to load the raw images to be measured first. Cannot currently save out `intensity_image` parameter to object file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4548905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/03/30 07:10:06 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/03/30 07:10:06 PM] Found intensity_image data\n",
      "[INFO][2022/03/30 07:10:06 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/03/30 07:24:37 PM] Objects are of type: <class 'dict'>\n",
      "[INFO][2022/03/30 07:24:40 PM] ...Found 446249 objects in 1098 frames.\n"
     ]
    }
   ],
   "source": [
    "detailed_objects = btrack.utils.segmentation_to_objects(\n",
    "    images['mask_2ch'], \n",
    "    images['gfp'],\n",
    "    properties = ('area', 'mean_intensity', 'intensity_image'), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63775a4c",
   "metadata": {},
   "source": [
    "Example image showing PCNA-iRFP morphology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(detailed_objects[0].properties['intensity_image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08114355",
   "metadata": {},
   "source": [
    "## 2b. Differentiate the objects based on class ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583c2fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_gfp = [obj for obj in objects if obj.properties['class id'] == 1]\n",
    "objects_rfp = [obj for obj in objects if obj.properties['class id'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc05b98",
   "metadata": {},
   "source": [
    "## 3. Classify the objects "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1725583",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10cd85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../models/cellx_classifier_stardist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce619f",
   "metadata": {},
   "source": [
    "Define normalisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d135ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channels(x):\n",
    "\n",
    "    for dim in range(x.shape[-1]):\n",
    "        x[..., dim] = normalize(x[..., dim])\n",
    "        \n",
    "    return x\n",
    "\n",
    "def normalize(x):\n",
    "\n",
    "    xf = x.astype(np.float32)\n",
    "    mx = np.mean(xf)\n",
    "    sd = np.max([np.std(xf), 1./np.prod(x.shape)])\n",
    "\n",
    "    return (xf - mx) / sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8428f",
   "metadata": {},
   "source": [
    "Define classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41184a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_objects(image,  gfp, rfp, objects, obj_type):\n",
    "    \n",
    "    # define stages of cell cycle to classify (dependent on model type)\n",
    "    LABELS = [\"interphase\", \"prometaphase\", \"metaphase\", \"anaphase\", \"apoptosis\"]\n",
    "    \n",
    "    # iterate over frames\n",
    "    for n in tqdm(range(image.shape[0])):\n",
    "        \n",
    "        # only select objects if in frame\n",
    "        _objects = [o for o in objects if o.t == n]\n",
    "        \n",
    "        # empty placeholder arrays\n",
    "        crops = []\n",
    "        to_update = []\n",
    "        \n",
    "        # select h2b channel to aid in classification\n",
    "        fp = gfp if obj_type == 1 else rfp\n",
    "        \n",
    "        # create stack by computing each frame of dask array input\n",
    "        frame = np.stack(\n",
    "            [image[n, ...].compute(), fp[n, ...].compute()], \n",
    "            axis=-1,) \n",
    "        \n",
    "        # create padded image for network\n",
    "        vol = InfinitePaddedImage(frame, mode = 'reflect')\n",
    "        \n",
    "        # iterate over objects \n",
    "        for obj in _objects:\n",
    "            \n",
    "            # create coords for image slice\n",
    "            xs = slice(int(obj.x-40), int(obj.x+40), 1)\n",
    "            ys = slice(int(obj.y-40), int(obj.y+40), 1)\n",
    "            \n",
    "            # crop image\n",
    "            crop = vol[ys, xs, :]\n",
    "            crop = resize(crop, (64, 64), preserve_range=True).astype(np.float32)\n",
    "            \n",
    "            # normalise image\n",
    "            if crop.shape == (64 ,64, 2):\n",
    "                crops.append(normalize_channels(crop))\n",
    "                to_update.append(obj)\n",
    "            else:\n",
    "                print(crop.shape)\n",
    "                \n",
    "        if not crops:\n",
    "            continue\n",
    "            \n",
    "        # use classifcation model to predict\n",
    "        pred = model.predict(np.stack(crops, axis=0))\n",
    "        \n",
    "        # check correct number of predictions\n",
    "        assert pred.shape[0] == len(_objects)\n",
    "        \n",
    "        # assign labels to objects\n",
    "        for idx in range(pred.shape[0]):\n",
    "            obj = _objects[idx]\n",
    "            \n",
    "            # assigning details of prediction\n",
    "            pred_label = np.argmax(pred[idx, ...])\n",
    "            pred_softmax = softmax(pred[idx, ...])\n",
    "            logits = {f\"prob_{k}\": pred_softmax[ki] for ki, k in enumerate(LABELS)}\n",
    "            \n",
    "            # write out\n",
    "            obj.label = pred_label\n",
    "            obj.properties = logits\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8781b4",
   "metadata": {},
   "source": [
    "#### Load raw images for classifier, a colour channel dependent on `obj_type` needed too (i.e. GFP is `obj_type = 1`, RFP is `obj_type = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657771d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = images['brightfield']\n",
    "gfp = images['gfp']\n",
    "rfp = images['rfp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30835a86",
   "metadata": {},
   "source": [
    "#### Classify objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_gfp = classify_objects(bf, gfp, rfp, objects_gfp, obj_type = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c8f9305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc82cfb95584a8dbd57e577df0d2bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects_rfp = classify_objects(bf, gfp, rfp, objects_rfp, obj_type = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70646be",
   "metadata": {},
   "source": [
    "#### Inspect an example object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec4f611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "      <th>class id</th>\n",
       "      <th>prob_interphase</th>\n",
       "      <th>prob_prometaphase</th>\n",
       "      <th>prob_metaphase</th>\n",
       "      <th>prob_anaphase</th>\n",
       "      <th>prob_apoptosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>160.435897</td>\n",
       "      <td>4.226496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>234</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.944189e-10</td>\n",
       "      <td>1.602896e-10</td>\n",
       "      <td>2.379132e-11</td>\n",
       "      <td>9.544462e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 0, 'x': 160.43589743589743, 'y': 4.226495726495727, 'z': 0.0, 't': 0, 'dummy': False, 'states': 0, 'label': 0, 'prob': 0.0, 'area': 234, 'class id': 1, 'prob_interphase': 1.0, 'prob_prometaphase': 1.9441894e-10, 'prob_metaphase': 1.602896e-10, 'prob_anaphase': 2.3791316e-11, 'prob_apoptosis': 9.544462e-10}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_gfp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5c716",
   "metadata": {},
   "source": [
    "#### Save out classified GFP objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796afd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/03/30 01:29:58 PM] Opening HDF file: /home/nathan/data/kraken/ras/ND0022/Pos12/objects_type_1.h5...\n",
      "[INFO][2022/03/30 01:30:03 PM] Writing objects/obj_type_1\n",
      "[INFO][2022/03/30 01:30:03 PM] Writing labels/obj_type_1\n",
      "[INFO][2022/03/30 01:30:03 PM] Loading objects/obj_type_1 (386145, 5) (386145 filtered: None)\n",
      "[INFO][2022/03/30 01:30:08 PM] Writing properties/obj_type_1/area (386145,)\n",
      "[INFO][2022/03/30 01:30:08 PM] Writing properties/obj_type_1/class id (386145,)\n",
      "[INFO][2022/03/30 01:30:08 PM] Writing properties/obj_type_1/prob_interphase (386145,)\n",
      "[INFO][2022/03/30 01:30:08 PM] Writing properties/obj_type_1/prob_prometaphase (386145,)\n",
      "[INFO][2022/03/30 01:30:08 PM] Writing properties/obj_type_1/prob_metaphase (386145,)\n",
      "[INFO][2022/03/30 01:30:08 PM] Writing properties/obj_type_1/prob_anaphase (386145,)\n",
      "[INFO][2022/03/30 01:30:08 PM] Writing properties/obj_type_1/prob_apoptosis (386145,)\n",
      "[INFO][2022/03/30 01:30:08 PM] Closing HDF file: /home/nathan/data/kraken/ras/ND0022/Pos12/objects_type_1.h5\n"
     ]
    }
   ],
   "source": [
    "with btrack.dataio.HDF5FileHandler(\n",
    "    f'{root_dir}/{expt}/{pos}/objects_type_1.h5', 'w', obj_type='obj_type_1',\n",
    ") as hdf:\n",
    "    #hdf.write_segmentation(masks['mask'])\n",
    "    hdf.write_objects(objects_gfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb0274",
   "metadata": {},
   "source": [
    "#### Save out classified RFP objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71de0f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/03/30 02:38:14 PM] Opening HDF file: /home/nathan/data/kraken/ras/ND0022/Pos12/objects_type_2.h5...\n",
      "[INFO][2022/03/30 02:38:15 PM] Writing objects/obj_type_2\n",
      "[INFO][2022/03/30 02:38:15 PM] Writing labels/obj_type_2\n",
      "[INFO][2022/03/30 02:38:15 PM] Loading objects/obj_type_2 (38765, 5) (38765 filtered: None)\n",
      "[INFO][2022/03/30 02:38:15 PM] Writing properties/obj_type_2/area (38765,)\n",
      "[INFO][2022/03/30 02:38:16 PM] Writing properties/obj_type_2/class id (38765,)\n",
      "[INFO][2022/03/30 02:38:16 PM] Writing properties/obj_type_2/prob_interphase (38765,)\n",
      "[INFO][2022/03/30 02:38:16 PM] Writing properties/obj_type_2/prob_prometaphase (38765,)\n",
      "[INFO][2022/03/30 02:38:16 PM] Writing properties/obj_type_2/prob_metaphase (38765,)\n",
      "[INFO][2022/03/30 02:38:16 PM] Writing properties/obj_type_2/prob_anaphase (38765,)\n",
      "[INFO][2022/03/30 02:38:16 PM] Writing properties/obj_type_2/prob_apoptosis (38765,)\n",
      "[INFO][2022/03/30 02:38:16 PM] Closing HDF file: /home/nathan/data/kraken/ras/ND0022/Pos12/objects_type_2.h5\n"
     ]
    }
   ],
   "source": [
    "with btrack.dataio.HDF5FileHandler(\n",
    "    f'{root_dir}/{expt}/{pos}/objects_type_2.h5', 'w', obj_type='obj_type_2',\n",
    ") as hdf:\n",
    "    #hdf.write_segmentation(masks['mask'])\n",
    "    hdf.write_objects(objects_rfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e1448",
   "metadata": {},
   "source": [
    "# Saving out as one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3e2b7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/03/30 06:27:43 PM] Opening HDF file: /home/nathan/data/kraken/ras/ND0022/Pos12/objects.h5...\n",
      "[INFO][2022/03/30 06:27:47 PM] Writing objects/obj_type_1\n",
      "[INFO][2022/03/30 06:27:47 PM] Writing labels/obj_type_1\n",
      "[INFO][2022/03/30 06:27:47 PM] Loading objects/obj_type_1 (386145, 5) (386145 filtered: None)\n",
      "[INFO][2022/03/30 06:27:52 PM] Writing properties/obj_type_1/area (386145,)\n",
      "[INFO][2022/03/30 06:27:52 PM] Writing properties/obj_type_1/class id (386145,)\n",
      "[INFO][2022/03/30 06:27:52 PM] Writing properties/obj_type_1/prob_interphase (386145,)\n",
      "[INFO][2022/03/30 06:27:52 PM] Writing properties/obj_type_1/prob_prometaphase (386145,)\n",
      "[INFO][2022/03/30 06:27:52 PM] Writing properties/obj_type_1/prob_metaphase (386145,)\n",
      "[INFO][2022/03/30 06:27:53 PM] Writing properties/obj_type_1/prob_anaphase (386145,)\n",
      "[INFO][2022/03/30 06:27:53 PM] Writing properties/obj_type_1/prob_apoptosis (386145,)\n",
      "[INFO][2022/03/30 06:27:53 PM] Closing HDF file: /home/nathan/data/kraken/ras/ND0022/Pos12/objects.h5\n",
      "[INFO][2022/03/30 06:27:53 PM] Opening HDF file: /home/nathan/data/kraken/ras/ND0022/Pos12/objects.h5...\n",
      "[INFO][2022/03/30 06:27:53 PM] Writing objects/obj_type_2\n",
      "[INFO][2022/03/30 06:27:53 PM] Writing labels/obj_type_2\n",
      "[INFO][2022/03/30 06:27:53 PM] Loading objects/obj_type_2 (38765, 5) (38765 filtered: None)\n",
      "[INFO][2022/03/30 06:27:54 PM] Writing properties/obj_type_2/area (38765,)\n",
      "[INFO][2022/03/30 06:27:54 PM] Writing properties/obj_type_2/class id (38765,)\n",
      "[INFO][2022/03/30 06:27:54 PM] Writing properties/obj_type_2/prob_interphase (38765,)\n",
      "[INFO][2022/03/30 06:27:54 PM] Writing properties/obj_type_2/prob_prometaphase (38765,)\n",
      "[INFO][2022/03/30 06:27:54 PM] Writing properties/obj_type_2/prob_metaphase (38765,)\n",
      "[INFO][2022/03/30 06:27:54 PM] Writing properties/obj_type_2/prob_anaphase (38765,)\n",
      "[INFO][2022/03/30 06:27:54 PM] Writing properties/obj_type_2/prob_apoptosis (38765,)\n",
      "[INFO][2022/03/30 06:27:54 PM] Closing HDF file: /home/nathan/data/kraken/ras/ND0022/Pos12/objects.h5\n"
     ]
    }
   ],
   "source": [
    "with btrack.dataio.HDF5FileHandler(\n",
    "    f'{root_dir}/{expt}/{pos}/objects.h5', 'w', obj_type='obj_type_1',\n",
    ") as hdf:\n",
    "    #hdf.write_segmentation(masks['mask'])\n",
    "    hdf.write_objects(objects_gfp)\n",
    "    \n",
    "with btrack.dataio.HDF5FileHandler(\n",
    "    f'{root_dir}/{expt}/{pos}/objects.h5', 'a', obj_type='obj_type_2',\n",
    ") as hdf:\n",
    "    #hdf.write_segmentation(masks['mask'])\n",
    "    hdf.write_objects(objects_rfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e10e7",
   "metadata": {},
   "source": [
    "# 4. Batch process\n",
    "Iterate over many experiments and positions (need to ensure you define normalisation and classification functions above first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a28d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b7b01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(14400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1792133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348a770fee084ddeaa898748e8bc58e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0037e32b3d4aa59f7a36a4f54be8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ND0013/Pos0\n",
      "Using cropping: (1200, 1600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/03/23 07:02:56 PM] Localizing objects from segmentation...\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "root_dir = '/home/nathan/data/kraken/ras'\n",
    "expt_list = sorted([expt for expt in os.listdir(root_dir) \n",
    "                    if 'ND' in expt and os.path.isdir(os.path.join(root_dir, expt))\n",
    "                    and not '21' in expt \n",
    "                    and not '20' in expt], \n",
    "                    key = lambda x: [int(y) for y in re.findall(r'\\d+', x)])\n",
    "pos_list = 'all'\n",
    "overwrite = True\n",
    "\n",
    "for expt in tqdm(expt_list):\n",
    "    \n",
    "    # Find all positions in that experiment, if pos_list is all then it finds all positions\n",
    "    if pos_list == 'all':\n",
    "        pos_list = sorted([pos for pos in os.listdir(f'{root_dir}/{expt}') \n",
    "                    if 'Pos' in pos \n",
    "                    and os.path.isdir(f'{root_dir}/{expt}/{pos}')], \n",
    "                    key = lambda x: [int(y) for y in re.findall(r'\\d+', x)])  \n",
    "\n",
    "    ### Iterate over all positions in that experiment\n",
    "    for pos in tqdm(pos_list):\n",
    "\n",
    "        ### check if overwrite param is false check if raw directory already created and if type of transform file already exists and decide whether to skip pos\n",
    "        if not overwrite and glob.glob(f'{root_dir}/{expt}/{pos}/*objects*.h5'):\n",
    "            print(glob.glob(f'{root_dir}/{expt}/{pos}/*objects*.h5'), f'file found, skipping {expt}/{pos}')\n",
    "            continue\n",
    "\n",
    "        print(f'Starting {expt}/{pos}')\n",
    "        # load segmentation images and apply necessary transforms and crops\n",
    "        image_path = f'{root_dir}/{expt}/{pos}/{pos}_images'\n",
    "        transform_path = f'{root_dir}/{expt}/{pos}/mask_reversed_clipped_transform_tensor.npy' #gfp_transform_tensor.npy'\n",
    "        images = DaskOctopusLiteLoader(image_path, \n",
    "                           transforms=transform_path,\n",
    "                           crop=(1200,1600), \n",
    "                           remove_background=False)\n",
    "        \n",
    "        # ID the objects in each segmentation image and assign option properties to them\n",
    "        objects = btrack.utils.segmentation_to_objects(\n",
    "                                                        images['mask'], \n",
    "                                                        properties = ('area',),\n",
    "                                                        assign_class_ID = True\n",
    "        )\n",
    "\n",
    "        # differentiate the objects based on class ID\n",
    "        objects_gfp = [obj for obj in objects if obj.properties['class id'] == 1]\n",
    "        objects_rfp = [obj for obj in objects if obj.properties['class id'] == 2]\n",
    "\n",
    "        # load classifcation model and define labels\n",
    "        model = load_model('../models/cellx_classifier_stardist.h5')\n",
    "        LABELS = [\"interphase\", \"prometaphase\", \"metaphase\", \"anaphase\", \"apoptosis\"]\n",
    "        \n",
    "        # load images for classifcation\n",
    "        bf = images['brightfield']\n",
    "        gfp = images['gfp']\n",
    "        rfp = images['rfp']\n",
    "\n",
    "        # classify objects\n",
    "        print(\"Classifying objects\")\n",
    "        objects_gfp = classify_objects(bf, gfp, rfp, objects_gfp, obj_type = 1)\n",
    "        objects_rfp = classify_objects(bf, gfp, rfp, objects_rfp, obj_type = 2)\n",
    "\n",
    "        # save out classified objects as segmentation h5 file\n",
    "        with btrack.dataio.HDF5FileHandler(\n",
    "            f'{root_dir}/{expt}/{pos}/objects_type_1.h5', 'w', obj_type='obj_type_1',\n",
    "        ) as hdf:\n",
    "            #hdf.write_segmentation(masks['mask'])\n",
    "            hdf.write_objects(objects_gfp)\n",
    "        with btrack.dataio.HDF5FileHandler(\n",
    "            f'{root_dir}/{expt}/{pos}/objects_type_2.h5', 'w', obj_type='obj_type_2',\n",
    "        ) as hdf:\n",
    "            #hdf.write_segmentation(masks['mask'])\n",
    "            hdf.write_objects(objects_rfp)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d258653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nathan/data/kraken/ras/ND0013/Pos3/mask_reversed_transform_tensor_clipped.npy'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c65d868e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.00000000e+00,  0.00000000e+00, -6.22149428e+01],\n",
       "        [ 0.00000000e+00,  1.00000000e+00,  1.00000000e+01],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00]],\n",
       "\n",
       "       [[ 1.00000000e+00,  0.00000000e+00, -5.46285883e+01],\n",
       "        [ 0.00000000e+00,  1.00000000e+00,  1.00000000e+01],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00]],\n",
       "\n",
       "       [[ 1.00000000e+00,  0.00000000e+00, -5.72211371e+01],\n",
       "        [ 0.00000000e+00,  1.00000000e+00,  1.00000000e+01],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.00000000e+00,  0.00000000e+00, -5.03624896e-03],\n",
       "        [ 0.00000000e+00,  1.00000000e+00, -7.44898619e-01],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00]],\n",
       "\n",
       "       [[ 1.00000000e+00,  0.00000000e+00,  2.17864873e+00],\n",
       "        [ 0.00000000e+00,  1.00000000e+00,  9.02855882e-02],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00]],\n",
       "\n",
       "       [[ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(transform_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dff61c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cropping: (1200, 1600)\n"
     ]
    }
   ],
   "source": [
    "images = DaskOctopusLiteLoader(image_path, \n",
    "                           transforms=transform_path,\n",
    "                           crop=(1200,1600), \n",
    "                           remove_background=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e99e9a",
   "metadata": {},
   "source": [
    "# Parallel batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eb3bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(pos):\n",
    "    ### check if overwrite param is false check if raw directory already created and if type of transform file already exists and decide whether to skip pos\n",
    "    if not overwrite and glob.glob(f'{root_dir}/{expt}/{pos}/*objects*.h5'):\n",
    "        print(glob.glob(f'{root_dir}/{expt}/{pos}/*objects*.h5'), f'file found, skipping {expt}/{pos}')\n",
    "        return\n",
    "\n",
    "    print(f'Starting {expt}/{pos}')\n",
    "    # load segmentation images and apply necessary transforms and crops\n",
    "    image_path = f'{root_dir}/{expt}/{pos}/{pos}_images'\n",
    "    transform_path = f'{root_dir}/{expt}/{pos}/gfp_transform_tensor.npy'\n",
    "    images = DaskOctopusLiteLoader(image_path, \n",
    "                       transforms=transform_path,\n",
    "                       crop=(1200,1600), \n",
    "                       remove_background=False)\n",
    "\n",
    "    # ID the objects in each segmentation image and assign option properties to them\n",
    "    objects = btrack.utils.segmentation_to_objects(\n",
    "        images['mask'], images['mask'],\n",
    "        properties = ('area', 'max_intensity', ),\n",
    "    )\n",
    "\n",
    "    # differentiate the objects based on class ID\n",
    "    objects_gfp = [obj for obj in objects if obj.properties['max_intensity'] == 1]\n",
    "    objects_rfp = [obj for obj in objects if obj.properties['max_intensity'] == 2]\n",
    "\n",
    "    # load classifcation model and define labels\n",
    "    model = load_model('../models/cellx_classifier_stardist.h5')\n",
    "    LABELS = [\"interphase\", \"prometaphase\", \"metaphase\", \"anaphase\", \"apoptosis\"]\n",
    "\n",
    "    # load images for classifcation\n",
    "    bf = images['brightfield']\n",
    "    gfp = images['gfp']\n",
    "    rfp = images['rfp']\n",
    "\n",
    "    # classify objects\n",
    "    print(\"Classifying objects\")\n",
    "    objects_gfp = classify_objects(bf, objects_gfp, obj_type = 1)\n",
    "    objects_rfp = classify_objects(bf, objects_rfp, obj_type = 2)\n",
    "\n",
    "    # save out classified objects as segmentation h5 file\n",
    "    with btrack.dataio.HDF5FileHandler(\n",
    "        f'{root_dir}/{expt}/{pos}/objects_type_1.h5', 'w', obj_type='obj_type_1',\n",
    "    ) as hdf:\n",
    "        #hdf.write_segmentation(masks['mask'])\n",
    "        hdf.write_objects(objects_gfp)\n",
    "    with btrack.dataio.HDF5FileHandler(\n",
    "        f'{root_dir}/{expt}/{pos}/objects_type_2.h5', 'w', obj_type='obj_type_2',\n",
    "    ) as hdf:\n",
    "        #hdf.write_segmentation(masks['mask'])\n",
    "        hdf.write_objects(objects_rfp)     \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc24d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "cpus = os.cpu_count()\n",
    "cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e276356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pos5',\n",
       " 'Pos11',\n",
       " 'Pos3',\n",
       " 'Pos1',\n",
       " 'Pos8',\n",
       " 'Pos10',\n",
       " 'Pos0',\n",
       " 'Pos2',\n",
       " 'Pos6',\n",
       " 'Pos7',\n",
       " 'Pos9',\n",
       " 'Pos4']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = [pos for pos in os.listdir(f'{root_dir}/{expt}') \n",
    "                    if 'Pos' in pos \n",
    "                    and os.path.isdir(f'{root_dir}/{expt}/{pos}')]\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74aeb131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ND0010/Pos3Starting ND0010/Pos4Starting ND0010/Pos11Starting ND0010/Pos5Starting ND0010/Pos7Starting ND0010/Pos1Starting ND0010/Pos10Starting ND0010/Pos0Starting ND0010/Pos8\n",
      "Starting ND0010/Pos2\n",
      "Starting ND0010/Pos9\n",
      "\n",
      "Starting ND0010/Pos6\n",
      "\n",
      "\n",
      "Using cropping: (1200, 1600)Using cropping: (1200, 1600)\n",
      "\n",
      "Using cropping: (1200, 1600)Using cropping: (1200, 1600)Using cropping: (1200, 1600)Using cropping: (1200, 1600)\n",
      "Using cropping: (1200, 1600)\n",
      "\n",
      "Using cropping: (1200, 1600)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Using cropping: (1200, 1600)\n",
      "Using cropping: (1200, 1600)\n",
      "\n",
      "Using cropping: (1200, 1600)\n",
      "\n",
      "Using cropping: (1200, 1600)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "[INFO][2022/02/07 05:18:20 PM] Localizing objects from segmentation...\n",
      "[INFO][2022/02/07 05:18:20 PM] Found intensity_image data\n",
      "[INFO][2022/02/07 05:18:20 PM] Calculating weighted centroids using intensity_image\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-21:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f2b6c6875a03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/analysis/miniconda3/envs/cellx/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analysis/miniconda3/envs/cellx/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analysis/miniconda3/envs/cellx/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analysis/miniconda3/envs/cellx/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analysis/miniconda3/envs/cellx/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for expt in expt_list:\n",
    "    if __name__ == '__main__':\n",
    "        with Pool(cpus) as p:\n",
    "            p.map(classify, pos_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CellX",
   "language": "python",
   "name": "cellx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
